{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据的neo4j Cypher语句：\n",
    "\n",
    "```\n",
    "CREATE (a1:很搞笑的理由 {值:'游回来'}) - [:属于 {前提条件_当证券代码等于:'002069'}] -> (b1:存在异常 {值:'很搞笑的理由'}) <-  [:属于 {前提条件_当证券代码等于:'002069'}] - (c1:很搞笑的理由 {值:'逃跑'});\n",
    "\n",
    "match (x:很搞笑的理由 {值:'逃跑'})\n",
    "CREATE (a2:输入词 {值:'死亡'}) - [:归类为 {前提条件_当证券代码等于:'002069'}] -> (x) <-  [:归类为 {前提条件_当证券代码等于:'002069'}] - (c2:输入词 {值:'异常'});\n",
    "\n",
    "match data=(a3:存在异常) -[*]-(b3)\n",
    "return data;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.converter import TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "except:\n",
    "    %pip install pdfminer\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.converter import TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "    from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "def 读取pdf(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#遍历pdf目录找出所有pdf文件\n",
    "import os\n",
    "def 遍历pdf目录找出所有pdf文件(folder):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdf\\\\2014年10月30日 关于海洋牧场灾情说明会的公告.PDF',\n",
       " 'pdf\\\\2018年2月05日 关于底播虾夷扇贝2017年终盘点情况的公告.PDF',\n",
       " 'pdf\\\\2019年11月11日 关于2019年秋季底播虾夷扇贝存量抽测的风险提示公告.PDF',\n",
       " 'pdf\\\\第五届董事会第十七次会议决议公告.PDF']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取当前目录\n",
    "import os\n",
    "def 获取当前目录():\n",
    "    return os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "pdf文件列表 = 遍历pdf目录找出所有pdf文件('pdf')\n",
    "pdf文件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 获取pdf文件的文件名(pdf文件):\n",
    "    pdf文件名 = pdf文件.split('\\\\')[-1]\n",
    "    return pdf文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014年10月30日 关于海洋牧场灾情说明会的公告.PDF'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "获取pdf文件的文件名(pdf文件列表[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo4j数据库操作相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://1144f8d0.databases.neo4j.io:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"VPrCeTO-8CzYElUogkj45D4Ec0x454oBgAqHIjHZG4k\")) #enter your neo4j username and password instead of 'test'\n",
    "session = driver.session()\n",
    "\n",
    "#define relevant functions to execute differnet queries\n",
    "def run_query(tx, query, 证券代码,输入词列表):\n",
    "    result_list=[]\n",
    "    for record in tx.run(query, 证券代码=证券代码,输入词列表=输入词列表):\n",
    "        result_list.append(record[0])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知识推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "图数据库查询用语句 = (\"match (a:存在异常) -[b:属于 {前提条件_当证券代码等于:$证券代码}]-(c)-[d:归类为 {前提条件_当证券代码等于:$证券代码}] -(e:输入词) where e.值 in $输入词列表 return a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可解释相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'因为该上市公司于2014年10月30日发生过类似的事情，参考信息：https://zhuanlan.zhihu.com/p/382126195;https://www.sohu.com/a/474688580_473133'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.w3school.com.cn/python/ref_string_format.asp\n",
    "可解释信息 = \"因为该上市公司于{上次发生日期}发生过类似的事情，参考信息：{参考信息链接列表}\".format(上次发生日期 = \"2014年10月30日\", 参考信息链接列表 = \"https://zhuanlan.zhihu.com/p/382126195;https://www.sohu.com/a/474688580_473133\")\n",
    "可解释信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "证券代码列表 = [\"002069\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载nlp模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\billy\\Desktop\\demo\\Hands-On-Artificial-Intelligence-for-Banking-Chinese\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print('加载nlp模型')\n",
    "import spacy\n",
    "nlp = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\billy\\Desktop\\demo\\Hands-On-Artificial-Intelligence-for-Banking-Chinese\\env\\lib\\site-packages\\spacy\\pipeline\\entityruler.py:334: UserWarning: [W012] A Doc object you're adding to the PhraseMatcher for pattern '证券代码' is parsed and/or tagged, but to match on 'ORTH', you don't actually need this information. This means that creating the patterns is potentially much slower, because all pipeline components are applied. To only create tokenized Doc objects, try using `nlp.make_doc(text)` or process all texts as a stream using `list(nlp.tokenizer.pipe(all_texts))`.\n",
      "  self.phrase_matcher.add(label, [pattern])  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\",config={\"validate\": True,\"overwrite_ents\": True})\n",
    "patterns = [{\"label\": \"证券代码\", \"pattern\": code} for code in 证券代码列表]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "是否输出详细信息 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 提取实体(doc):\n",
    "    print('AI开始思考。。。')\n",
    "    return doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 预测(pdf文件):\n",
    "    doc = nlp(读取pdf(pdf文件))\n",
    "    实体列表 = 提取实体(doc)\n",
    "    证券代码 = 实体列表[0].text\n",
    "    # print('证券代码=',证券代码)\n",
    "    # 证券代码 = '002069'\n",
    "    # 遍历doc中的所有token，将text属性值放入一个list\n",
    "    输入词列表 = [token.text for token in doc if token.text.strip() != '']\n",
    "    #将输入词列表写入到文件\n",
    "    pdf文件名 = 获取pdf文件的文件名(pdf文件)\n",
    "    with open('输入词列表' + pdf文件名 + '.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\",\"'.join(输入词列表))\n",
    "    results = session.read_transaction(run_query, 图数据库查询用语句, 证券代码,输入词列表)\n",
    "    if len(results) > 0:\n",
    "        return (True, 可解释信息)\n",
    "    else:\n",
    "        return (False, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI开始思考。。。\n",
      "2014年10月30日 关于海洋牧场灾情说明会的公告.PDF :  True 。相关可解释信息： 因为该上市公司于2014年10月30日发生过类似的事情，参考信息：https://zhuanlan.zhihu.com/p/382126195;https://www.sohu.com/a/474688580_473133\n",
      "AI开始思考。。。\n",
      "2018年2月05日 关于底播虾夷扇贝2017年终盘点情况的公告.PDF :  True 。相关可解释信息： 因为该上市公司于2014年10月30日发生过类似的事情，参考信息：https://zhuanlan.zhihu.com/p/382126195;https://www.sohu.com/a/474688580_473133\n",
      "AI开始思考。。。\n",
      "2019年11月11日 关于2019年秋季底播虾夷扇贝存量抽测的风险提示公告.PDF :  True 。相关可解释信息： 因为该上市公司于2014年10月30日发生过类似的事情，参考信息：https://zhuanlan.zhihu.com/p/382126195;https://www.sohu.com/a/474688580_473133\n",
      "AI开始思考。。。\n",
      "第五届董事会第十七次会议决议公告.PDF :  False 。相关可解释信息： \n"
     ]
    }
   ],
   "source": [
    "#遍历pdf文件列表，预测每个pdf文件\n",
    "for pdf文件 in pdf文件列表:\n",
    "    预测结果 = 预测(pdf文件)\n",
    "    print(获取pdf文件的文件名(pdf文件), ': ', 预测结果[0],'。相关可解释信息：', 预测结果[1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a63688bd6bac23ef02bd6d37e471970d737fd4f4530cb65b7bef3153fa56e0ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
